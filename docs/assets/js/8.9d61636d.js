(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{152:function(t,s,a){"use strict";a.r(s);var n=a(0),e=Object(n.a)({},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"content"},[t._m(0),t._m(1),t._m(2),a("p",[t._v("A Chevrotain Lexer will by default track the full position information for each token.\nThis includes line and column information.")]),a("p",[t._v("In order to support this the Lexer must be aware of which Tokens may include line terminators.\nThis information must be provided by the lexer's author.")]),a("p",[t._v("This error means that the Lexer has been defined to track line and column information (perhaps by default).\nYet not a single one of the Token definitions passed to it was defined as possibly containing line terminators.")]),a("p",[t._v("To resolve this choose one of the following:")]),a("ol",[a("li",[a("p",[t._v("Disable the line and column position tracking using the "),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_4_0/interfaces/ilexerconfig.html#positiontracking",target:"_blank",rel:"noopener noreferrer"}},[t._v("positionTracking"),a("OutboundLink")],1),t._v(" configuration option.")]),t._m(3)]),a("li",[a("p",[t._v("Mark the Tokens which may include a line terminator with a line_breaks flag.")]),t._m(4),a("ul",[a("li",[a("p",[t._v("Note that the definition of what constitutes a line terminator is controlled by the\n"),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_4_0/interfaces/ilexerconfig.html#lineTerminatorsPattern",target:"_blank",rel:"noopener noreferrer"}},[t._v("lineTerminatorsPattern"),a("OutboundLink")],1),t._v(" lexer configuration property.")])]),t._m(5)])])]),t._m(6),a("p",[t._v("A Token RegExp pattern used in a chevrotain lexer may not use the start/end of input anchors ('$' and '^').")]),t._m(7),a("p",[t._v("This will be checked for during the initialization of the lexer.\nUnfortunately, this validation can detect false positives when the anchor characters\nare used in certain regExp contexts, for example:")]),t._m(8),t._m(9),t._m(10),t._m(11),t._m(12),a("p",[t._v("For example:")]),t._m(13),t._m(14),a("p",[t._v("To resolve this simply re-arrange the order of Token types in the lexer\ndefinition such that the more specific Token types will be listed first.")]),t._m(15),t._m(16),t._m(17),a("p",[t._v("To resolve this second problem see how to prefer the "),a("strong",[t._v("longest match")]),t._v("\nas demonstrated in the "),a("a",{attrs:{href:"https://github.com/SAP/Chevrotain/blob/master/examples/lexer/keywords_vs_identifiers/keywords_vs_identifiers.js",target:"_blank",rel:"noopener noreferrer"}},[t._v("keywords vs identifiers example"),a("OutboundLink")],1)]),t._m(18),a("p",[t._v("The Chevrotain Lexer performs optimizations by filtering the potential token matchs\nusing the next "),a("a",{attrs:{href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/charCodeAt",target:"_blank",rel:"noopener noreferrer"}},[t._v("charCode"),a("OutboundLink")],1),t._v(" to be consumed.\nTo apply this optimization the first possible charCodes for "),a("strong",[t._v("every")]),t._v(" TokenType must be identified.")]),t._m(19),t._m(20),t._m(21),t._m(22),a("p",[t._v('If the use of these optimizations is desired and the startup resources cost is acceptable\nIt is possilbe to enable the optimizations by explicitly providing a "'),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_4_0/interfaces/itokenconfig.html#start_chars_hint",target:"_blank",rel:"noopener noreferrer"}},[t._v("start_chars_hint"),a("OutboundLink")],1),t._v('" property.\ne.g:')]),t._m(23),a("p",[t._v("Please Note that filling such an array "),a("a",{attrs:{href:"https://jsperf.com/fill-16-bits",target:"_blank",rel:"noopener noreferrer"}},[t._v("can take over 1ms"),a("OutboundLink")],1),t._v(" on a modern machine.\nSo if you are only parsing small inputs and/or starting a new process for each\nparser invocation the added initilization cost may be counter productive.")]),t._m(24),a("p",[t._v("The Chevrotain Lexer performs optimizations by filtering the potential token matchs\nusing the next "),a("a",{attrs:{href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/charCodeAt",target:"_blank",rel:"noopener noreferrer"}},[t._v("charCode"),a("OutboundLink")],1),t._v(" to be consumed.\nTo apply this optimization the first possible charCodes for "),a("strong",[t._v("every")]),t._v(" TokenType must be identified.")]),a("p",[t._v("This analysis is implemented using the "),a("a",{attrs:{href:"https://github.com/bd82/regexp-to-ast",target:"_blank",rel:"noopener noreferrer"}},[t._v("regexp-to-ast"),a("OutboundLink")],1),t._v(" library.\nThis error usally indicates a bug in the regexp-to-ast library.\nThe impact is that the optimization described above would become disabled.\nLexing and Parsing will still work correctly, only slower...")]),a("p",[t._v("Please open a bug for the "),a("a",{attrs:{href:"https://github.com/bd82/regexp-to-ast",target:"_blank",rel:"noopener noreferrer"}},[t._v("regexp-to-ast"),a("OutboundLink")],1),t._v(" library.\nThis issue can be "),a("strong",[t._v("worked around")]),t._v(' by explicitly providing a "'),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_4_0/interfaces/itokenconfig.html#start_chars_hint",target:"_blank",rel:"noopener noreferrer"}},[t._v("start_chars_hint"),a("OutboundLink")],1),t._v('" property.')]),t._m(25),t._m(26),a("p",[t._v("The Chevrotain Lexer performs optimizations by filtering the potential token matchs\nusing the next "),a("a",{attrs:{href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/charCodeAt",target:"_blank",rel:"noopener noreferrer"}},[t._v("charCode"),a("OutboundLink")],1),t._v(" to be consumed.\nTo apply this optimization the first possible charCodes for "),a("strong",[t._v("every")]),t._v(" TokenType must be identified.")]),a("p",[t._v("This analysis is implemented using the "),a("a",{attrs:{href:"https://github.com/bd82/regexp-to-ast",target:"_blank",rel:"noopener noreferrer"}},[t._v("regexp-to-ast"),a("OutboundLink")],1),t._v(" library.\nThis library currently does not support the "),a("a",{attrs:{href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/unicode",target:"_blank",rel:"noopener noreferrer"}},[t._v("unicode regexp flag"),a("OutboundLink")],1),t._v("\nThe impact is that the optimization described above would become disabled.\nLexing and Parsing will still work correctly, just slower...")]),a("p",[t._v("This issue can be "),a("strong",[t._v("worked around")]),t._v(' by explicitly providing a "'),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_4_0/interfaces/itokenconfig.html#start_chars_hint",target:"_blank",rel:"noopener noreferrer"}},[t._v("start_chars_hint"),a("OutboundLink")],1),t._v('" property.')]),t._m(27),t._m(28),t._m(29),t._m(30),a("p",[t._v("The Chevrotain Lexer performs optimizations by filtering the potential token matchs\nusing the next "),a("a",{attrs:{href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/charCodeAt",target:"_blank",rel:"noopener noreferrer"}},[t._v("charCode"),a("OutboundLink")],1),t._v(" to be consumed.\nTo apply this optimization the first possible charCodes for "),a("strong",[t._v("every")]),t._v(" TokenType must be identified.")]),a("p",[t._v("This information cannot be automatically computed for "),a("a",{attrs:{href:"https://sap.github.io/chevrotain/docs/guide/custom_token_patterns.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("custom token patterns"),a("OutboundLink")],1),t._v("\nand "),a("strong",[t._v("should")]),t._v(' therefore be explicitly provided using the "'),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_4_0/interfaces/itokenconfig.html#start_chars_hint",target:"_blank",rel:"noopener noreferrer"}},[t._v("start_chars_hint"),a("OutboundLink")],1),t._v('" property.')]),a("p",[t._v("For example:")]),t._m(31),a("p",[t._v('Providing the "'),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_4_0/interfaces/itokenconfig.html#start_chars_hint",target:"_blank",rel:"noopener noreferrer"}},[t._v("start_chars_hint"),a("OutboundLink")],1),t._v('" property is '),a("strong",[t._v("not")]),t._v(" mandatory.\nIt will only enable performance optimizations in the lexer.")])])},[function(){var t=this.$createElement,s=this._self._c||t;return s("h1",{attrs:{id:"resolving-lexer-errors"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#resolving-lexer-errors","aria-hidden":"true"}},[this._v("#")]),this._v(" Resolving Lexer Errors")])},function(){var t=this.$createElement,s=this._self._c||t;return s("ul",[s("li",[s("a",{attrs:{href:"#LINE_BREAKS"}},[this._v("No LINE_BREAKS Error.")])]),s("li",[s("a",{attrs:{href:"#ANCHORS"}},[this._v("Unexpected RegExp Anchor Error.")])]),s("li",[s("a",{attrs:{href:"#UNREACHABLE"}},[this._v("Token Can Never Be Matched.")])]),s("li",[s("a",{attrs:{href:"#COMPLEMENT"}},[this._v("Complement Sets cannot be automatically optimized.")])]),s("li",[s("a",{attrs:{href:"#REGEXP_PARSING"}},[this._v("Failed parsing < /.../ > Using the regexp-to-ast library.")])]),s("li",[s("a",{attrs:{href:"#UNICODE_OPTIMIZE"}},[this._v("The regexp unicode flag is not currently supported by the regexp-to-ast library.")])]),s("li",[s("a",{attrs:{href:"#CUSTOM_OPTIMIZE"}},[this._v("TokenType <...> is using a custom token pattern without providing <char_start_hint> parameter")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h2",{attrs:{id:"LINE_BREAKS"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#LINE_BREAKS","aria-hidden":"true"}},[this._v("#")]),this._v(" No LINE_BREAKS Error")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" myTokens "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("IntegerLiteral"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringLiteral"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" WhiteSpace "),a("span",{attrs:{class:"token comment"}},[t._v("/*, ... */")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" myLexer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("myTokens"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    positionTracking"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"onlyOffset"')]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" createToken "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createToken\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("// Using createToken API")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Whitespace "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Whitespace"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/\\s+/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    line_breaks"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("// or in ES2015 syntax with static properties")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("Whitespace")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Token")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\nWhitespace"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token constant"}},[t._v("PATTERN")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/\\s+/")]),t._v("\nWhitespace"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token constant"}},[t._v("LINE_BREAKS")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" myTokens "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("IntegerLiteral"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringLiteral"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" WhiteSpace "),a("span",{attrs:{class:"token comment"}},[t._v("/*, ... */")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" myLexer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("myTokens"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("li",[s("p",[this._v("Also note that multi-line tokens such as some types of comments and string literals tokens may contain\nline terminators, if your language includes such tokens they must also be marked with the line_breaks flag.")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h2",{attrs:{id:"ANCHORS"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#ANCHORS","aria-hidden":"true"}},[this._v("#")]),this._v(" Unexpected RegExp Anchor Error")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" createToken "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createToken\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("// Using createToken API")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Whitespace "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Integer"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// invalid pattern using both anchors")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/^\\d+$/")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" createToken "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createToken\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" semVer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"semVer"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v('// will match semantic versions such as: "1.0.2", "^0.3.9"')]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// inside a character set ([...]) the carat ('^') character does not act as an anchor.")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// yet it would still cause the validation to fail.")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/[~^]?\\d+\\.\\d+\\.\\d+/")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("// will throw an error")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("semVer"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("It is possible to workaround this problem by simply "),s("strong",[this._v("escaping")]),this._v(" the the offending carat or dollar sign.")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" semVer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"semVer"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/[~\\^]?\\d+\\.\\d+\\.\\d+/")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h2",{attrs:{id:"UNREACHABLE"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#UNREACHABLE","aria-hidden":"true"}},[this._v("#")]),this._v(" Token can never be matched")])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("This error means that A Token type can never be successfully matched as\na "),s("strong",[this._v("previous")]),this._v(" Token type in the lexer definition will "),s("strong",[this._v("always")]),this._v(" matched instead.\nThis happens because the default behavior of Chevrotain is to attempt to match\ntokens "),s("strong",[this._v("by the order")]),this._v(" described in the lexer definition.")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" ForKeyword "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"ForKeyword"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/for/")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Identifier "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Identifier"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/[a-zA-z]+/")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("// Will throw Token <ForKeyword> can never be matched...")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v('// Because the input "for" is also a valid identifier')]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("// and matching an identifier will be attempted first.")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" myLexer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Identifier"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ForKeyword"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("ul",[s("li",[this._v("Note that this validation is limited to simple patterns such as keywords\nThe more general case of any pattern being a strict subset of a preceding pattern\nwill require much more in depth RegExp analysis capabilities.")])])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token comment"}},[t._v("// Identifier is now listed as the last Token type.")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" myLexer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("ForKeyword"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Identifier"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("Note that the solution provided above will create a new problem.\nAny identifier "),s("strong",[this._v("starting with")]),this._v(' "for" will be lexed as '),s("strong",[this._v("two separate")]),this._v(" tokens,\na ForKeyword and an identifier. For example:")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" myLexer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("ForKeyword"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Identifier"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("// [")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v('//    {image:"for"}')]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v('//    {image:"ward"}')]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("// ]")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" tokensResult "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" myLexer"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("tokenize")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v('"forward"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h2",{attrs:{id:"COMPLEMENT"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#COMPLEMENT","aria-hidden":"true"}},[this._v("#")]),this._v(" Complement Sets cannot be automatically optimized")])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("When a TokenType pattern uses a regExp complement Set as a potential "),s("strong",[this._v("first")]),this._v(" character\nthe optimization is skipped as translating a complement set to a regular set requires too many cpu cycles\nduring the Lexer's initialization.")])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("For example an XML Text is defined by "),s("strong",[this._v("everything")]),this._v(" except a closing tag.")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" XMLText "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"XMLText"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/[^<&]+/")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("This means that there are "),s("strong",[this._v("65533")]),this._v(" (65535 - 2) possible starting charCodes\nFor an XMLText token.")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" hints "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" i "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" i "),a("span",{attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("65535")]),a("span",{attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" i"),a("span",{attrs:{class:"token operator"}},[t._v("++")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// 38 is '<' and 60 is '&'")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i "),a("span",{attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("38")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("||")]),t._v(" i "),a("span",{attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("60")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        hints"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("push")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" XMLText "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"XMLText"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/[^<&]+/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    start_chars_hint"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" hints\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h2",{attrs:{id:"REGEXP_PARSING"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#REGEXP_PARSING","aria-hidden":"true"}},[this._v("#")]),this._v(" Failed parsing < /.../ > Using the regexp-to-ast library")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Integer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Integer"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// lets assume that this pattern caused an error in regexp-to-ast")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/[1-9]\\d*/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// by explicitly providing the first possible characters of this pattern")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// the analysis by the regexp-to-ast library will be skipped")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// and the optimization can be enabled.")]),t._v("\n    start_chars_hint"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token string"}},[t._v('"1"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"2"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"3"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"4"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"5"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"6"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"7"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"8"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"9"')]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h2",{attrs:{id:"UNICODE_OPTIMIZE"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#UNICODE_OPTIMIZE","aria-hidden":"true"}},[this._v("#")]),this._v(" The regexp unicode flag is not currently supported by the regexp-to-ast library")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token comment"}},[t._v("// 'ðŸ’©' character")]),t._v("\n"),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"PileOfPoo"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// \\u{xxxxx} 32bit unicode escape can only be used with the /u flag enabled.")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/\\u{1F4A9}/u")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// The 'ðŸ’©' character is represented by surrogate pairs: '\\uD83D\\uDCA9'")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// the start_chars_hint should only be provided the first of the pair.")]),t._v("\n    start_chars_hint"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("55357")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("Another way to "),s("strong",[this._v("work around")]),this._v(" the issue is to define the pattern as a string literal.\nAs that kind can be trivially optimized.\nThis is naturally only relevant for simple patterns.\nFor example:")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"LCurley"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// note that the pattern is a string literal, not a regExp literal.")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"{"')]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h2",{attrs:{id:"CUSTOM_OPTIMIZE"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#CUSTOM_OPTIMIZE","aria-hidden":"true"}},[this._v("#")]),this._v(" TokenType <...> is using a custom token pattern without providing <char_start_hint> parameter")])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" IntegerToken "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"IntegerToken"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        exec"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" offset"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),a("span",{attrs:{class:"token comment"}},[t._v("/* ... */")]),t._v("\n        "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    start_chars_hint"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token string"}},[t._v('"1"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"2"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"3"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"4"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"5"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"6"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"7"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"8"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"9"')]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])}],!1,null,null,null);s.default=e.exports}}]);